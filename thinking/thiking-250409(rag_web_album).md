2025-4-9

今天看了几篇感兴趣的文章，收获了几个有意思的项目～

1. 大模型提示词优化器Prompt Optimizer

亮点： 支持多轮迭代改进\开源，支持本地部署

online: Prompt Optimizer

效果: 以政务客服提示词进行测试，待补充

2. RAG复杂查询的方案

作者：果冻人工智能

链接：https://juejin.cn/post/7488897639012220968

来源：稀土掘金

解决痛点: 在专业领域上，对于陈述不明确的问题，会增加llm推理的复杂度

rag弊端：

• 上下文有限：它们通常无法一次处理大量推理。

• 知识整合：很难将不同的数据片段结合起来形成完整的答案。（典型的介绍糖葫芦制作过程，上文写糖葫芦，下文制作过程不包含糖葫芦，而是主体为山楂）

• **复杂推理**：理解和应用隐含知识需要高级分析。

示例：

1. 经济趋势• 问题：“经济形势将如何影响公司的未来发展？”

• 背景：一系列财务报告。• 挑战：需要理解经济因素如何影响商业结果。

2. 数学谜题• 问题：“如何用数字 5、5、5 和 1 组合成 24？”

• 背景：一组类似谜题的示例和解决方案。

• 挑战：解决这个问题需要创造性思维和对谜题规则的理解。

3. 法律问题• 问题：“阿富汗的父母可以将他们的国籍传给在国外出生的孩子吗？”

• 背景：全球公民法律的数据库，例如 GLOBALCIT 数据集。

• 挑战：需要解释法律规则并将其应用于特定情况。



方案：
- 离线学习
1. few-shot+逐步扩大数据集
2. 建模学习者+评论者
3. 给定示例
4. RICP
5. buffer-of-thought：从原始数据提取固有经验用于复用(与第三点相似)
- 多策略融合
6. medprompt： cot+knn，即agent hospital,结合记录检索+经验检索进行推理

- 上下文学习(ICL)

难点：需要选对prompt，如果选错会有误导性

针对这一难点，有openicl框架、vote-k、auto-cot(对问题聚类后，再对这个问题生成不同推理链路，从中挑选出最优答案，避免贪婪解码导致答案错误)

- 任务分解(DIN-SQL)

- 模型微调(RFT)

轻微调方法：

1. 适配器微调Adapter Tuning
2. Prefix Tuning/ Prompt Tuning
3. 低秩适配：限制训练范围，减少参数量

该文章还介绍了一些专业领域的模型和数据集，比如医患对话数据集ChatDoctor、中国法律推理模型DISC-LawLLM


- 
虽然这些方法名字不一样——指导原则、经验、模板、规则……

但它们的目标都是一致的，就是找出有用的模式或规律，让模型能更有效地推理。

这些模式的来源可能是：

• 模型自己生成的解释（比如 MedPrompt，Buffer-of-Thought）

• 模型训练过程中的错误（比如 GL，RICP，Agent Hospital）

• 故意设计出来的错误（比如 LEAP）

• 有些规则适用于所有任务（比如 Agent Hospital，RICP），有些则是针对具体问题才调用的（比如 MedPrompt，Buffer-of-Thought）


